<div class="main-content" style="max-width: 900px; margin: 0 auto; padding: 20px; font-family: 'Lora', serif; line-height: 1.6; background-color: #f9f9f9; border-radius: 8px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);">

    <h1 style="color: #088; text-align: center; margin-bottom: 30px;">Why Do are we doing vector calculus ü§î?</h1>
    
    <section style="margin-bottom: 30px;">
        <h2 style="color: #333;">The Role of Vector Calculus üìê</h2>
        <p style="font-family: 'Roboto Mono', monospace;">In machine learning, we often aim to minimize a <strong>loss function</strong> ‚Äî a mathematical representation of how far our model's predictions are from the actual values. To minimize the loss, we need to understand how changes in the model parameters affect the loss function. This is where <strong>gradients</strong> come in.</p>
        <p style="font-family: 'Roboto Mono', monospace;"><strong>Vector calculus</strong> provides the tools to compute gradients. These gradients indicate the direction in which we need to change our parameters to reduce the loss function. Understanding these concepts is essential for optimization algorithms, like <strong>gradient descent</strong>, which are foundational in modern machine learning.</p>
    </section>
    
    <section style="margin-bottom: 30px;">
        <h2 style="color: #333;">Example 1: Gradient Descent ‚¨áÔ∏è</h2>
        <p style="font-family: 'Roboto Mono', monospace;">When training a machine learning model, such as a neural network, the goal is to minimize the loss function. This could be something like the mean squared error between predicted and actual values. To minimize the loss, we compute the <strong>gradient</strong> of the loss function with respect to each parameter in the model. The gradient is a vector pointing in the direction of the steepest increase in the loss function.</p>
        <p style="font-family: 'Roboto Mono', monospace;">To minimize the loss, we move in the opposite direction of the gradient ‚Äî a process called <strong>gradient descent</strong>. Vector calculus enables the efficient computation of these gradients, which are used to update the model‚Äôs parameters and improve its performance over time.</p>
    </section>
    
    <section style="margin-bottom: 30px;">
        <h2 style="color: #333;">Example 2: Backpropagation in Neural Networks üß†</h2>
        <p style="font-family: 'Roboto Mono', monospace;">In neural networks, vector calculus is critical during the training process, specifically for <strong>backpropagation</strong>. During backpropagation, the network computes the gradients of the loss with respect to the parameters of each layer, which are then used to adjust the weights. This process relies on the chain rule from calculus to determine how changes in one layer affect the output of the network.</p>
    </section>
    
    <section style="margin-bottom: 30px;">
        <h2 style="color: #333;">How This Relates to What You‚Äôve Learned üìö</h2>
        <p style="font-family: 'Roboto Mono', monospace;">Before diving into vector calculus, you have likely studied <strong>linear algebra</strong> and worked with <strong>systems of linear equations</strong>. These topics introduce vectors and matrices, which are essential for structuring data in machine learning. Vector calculus builds on this foundation by introducing concepts like gradients, which represent rates of change, and helping you solve optimization problems by adjusting model parameters.</p>
    </section>
    
    <section style="margin-bottom: 30px;">
        <h2 style="color: #333;">References</h2>
        <p style="font-family: 'Roboto Mono', monospace;"><a href="https://medium.com/@haytham99cheikhrouhou/dont-worry-about-it-part-2-vector-calculus-and-optimization-3f2c504a98de" style="color: #088;">Haytham Cheikhrouhou, "Don't Worry About It (Part 2): Vector Calculus and Optimization", Medium, 2020.</a></p>
    </section>
    
    </div>
    
